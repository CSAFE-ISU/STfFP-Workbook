# Statistical Preliminaries 

\vspace{-.1in}
Briefly, this section contains a broad review of probability concepts and of statistical inference concepts, with examples from the forensic science context. We will cover probability, data collection, statistical distributions, estimation, and hypothesis testing. \vspace{-.25in}

### Definitions

- **population**: ______________________________________________________

\vspace{.1in}

- **sample**: _____________________________________________________

\vspace{.1in}

- **probability**: Using knowledge about the ________________ to make statements describing the __________________. Probability can loosely be thought of as a type of deductive reasoning, where we are applying general knowledge about the population of interest to make conclusions about a small part of that population. 

\vspace{.1in}

- **statistics**: Using knowledge about the ________________ to make statements describing the __________________. Statistics can loosely be thought of as a type of inductive reasoning, where we are applying knowledge about a sample to state that something *may* be true about the population generally.

```{r thebigpic, echo=FALSE, out.width='.5\\linewidth', fig.align='center', fig.cap='"The Big Picture"', fig.pos="h"}
knitr::include_graphics('img/thebigpic.png')
```

### Forensic Science Examples

- Suppose 100 1-pound bags of cocaine are seized on the US-Mexico border, and the FBI want to know the chemical composition of the confiscated drugs to store in their database.\vspace{.1in}  
    * Population: __________________________
    \vspace{.1in}
    * Sample: __________________________
    \vspace{.1in}
- A window was broken in a robbery, and the suspect who was apprehended nearby had glass fragments lodged in the soles of their shoes. Do the fragments from the suspect's shoes have the same or similar chemical composition as the broken window? \vspace{.1in}  
    * Population 1: __________________________
    \vspace{.1in}
    * Sample 1: __________________________  
    \vspace{.1in}
    * Population 2: __________________________
    \vspace{.1in}
    * Sample 2: __________________________  
\vspace{.1in}
- A city government employee is suspected of embezzling funds from the city's coffers. Forensic accountants examine a subset of the city's transactions to determine whether embezzling occurred and how much money was lost. \vspace{.1in}  
    * Population: __________________________
    \vspace{.1in}
    * Sample: __________________________
    
How do you think this pertains to pattern evidence? List some possible relevant populations and samples below.\vspace{.1in}  

- Population 1: __________________________
    \vspace{.1in}
- Sample 1: __________________________  
    \vspace{.1in}
- Population 2: __________________________
    \vspace{.1in}
- Sample 2: __________________________ 
    \vspace{.1in}
- Population 3: __________________________
    \vspace{.1in}
- Sample 3: __________________________ 


## Probability

Probability concerns the *uncertainty* of outcomes. The set of all possible outcomes is called the ___________ space, and a particular outcome or set of outcomes of interest is referred to as an __________. 

### Examples 

1. Footwear 
    * Sample Space = All shoe sizes e.g. $\{6, 6.5, 7, 7.5, 8, 8.5, \dots\}$ 
    * Event = Shoe of size 9
2. Footwear
    * Sample Space = Brand of shoe e.g. \{ Nike, Vans, Converse, $\dots$\}
    * Event = Nike sneaker
3. Firearms
    * Sample Space = CMS (consecutive matching striae) for a pair of bullets e.g. $\{0, 1, 2, 3, 4, \dots \}$
    * Event = CMS of 10 or more 

### Interpretation

The probability of observing an event in a sample space is a number less than or equal to 1 and greater than or equal to 0 that describes the ____________ that the event will occur. 

There are two primary interpretations of probability: \vspace{.1in}

1. The long run ____________ of occurrence of an event. \vspace{.1in}
2. The _____________ belief of likelihood of an event occurring. 

### Basic Notation and Laws of Probability

Let an event of interest be denoted by _______. The probability of this event occurring is then denoted __________. Recall that the probability of an event is always between 0 and 1. When $P(Y) = 0$, the event $Y$ will never happen. When $P(Y) = 1$, the event $Y$ will always happen. The sum of the probabilities of all possbile outcomes in the sample space always equal to _____. 

The event of interest, $Y$, also has a complement event, $\overline{Y}$, which is read as "not $Y$". The complement, $\overline{Y}$, of an event, $Y$, is itself an event containing all outcomes in the sample space other than that initial event of interest, $Y$. 

$$ P(Y) + P(\overline{Y}) = \_\_\_$$ 

The above equation also gives us the following rules: 

\begin{equation}\label{eq:1}
\begin{split}
P(Y) & = 1 - P(\overline{Y}) \\
 P(\overline{Y}) & = 1 - P(Y)
\end{split}
\end{equation}

### Probability and Odds

The probability of an event defines the odds of the event. The odds *in favor* of an event $Y$ are defined as the probability of $Y$ divided by the probability of everything except $Y$ ("not $Y$"): 

$$ O(Y) = \dfrac{P(Y)}{P(\overline{Y})} = \dfrac{P(Y)}{1-\_\_}.$$ 

Conversely, the odds *against* a event $Y$ are defined as the porbability of everything except $Y$ ("not $Y$") divided by the probability of $Y$: 

$$ O(\overline{Y}) = \dfrac{P(\overline{Y})}{P(Y)} = \dfrac{1-\_\_}{P(Y)}.$$ 

When we typically talk about odds, like in horse racing, the odds reported are the odds *against* the outcome of interest. Let's construct a horse race scenario using our probability notation to find the probability of a horse winning a race from the reported odds: 

- Suppose you want to place a bet on a horse name Cleopatra winning the race. Odds for Cleopatra are reported as 4:1. 
- $Y$ = Cleopatra wins the race
- $\overline{Y}$ = Any horse in the race *other than* Cleopatra wins the race.
- $O(\overline{Y}) = \dfrac{P(\overline{Y})}{P(Y)} = \frac{4}{1} = 4$
- We know that $P(Y) + P(\overline{Y})=1$. With this information, we can determine $P(Y)$, which is the probability that Cleopatra wins the race: 
\begin{align*}
O(\overline{Y}) & = \dfrac{P(\overline{Y})}{P(Y)} = 4 \\
\Rightarrow  \dfrac{P(\overline{Y})}{P(Y)} & = 4 \\ \Rightarrow  \dfrac{1 - P(Y)}{P(Y)} & = 4 \quad \quad \textit{(See Equation \ref{eq:1})} \\ 
\Rightarrow \dfrac{1}{P(Y)} - 1 & = 4 \\
\Rightarrow \dfrac{1}{P(Y)} & = 5 \\
\Rightarrow P(Y) & = \frac{1}{5} = 0.2 \\
\Rightarrow P(\overline{Y}) & = 0.8
\end{align*}
- So, the odds for Cleopatra (4:1) mean that Cleopatra has a probability of 0.2 of winning the race. Because this outcome is not very likely (it will only happen in 1 race out of 5), you win money if Cleopatra wins simply because that is not a likely outcome. 
- **Betting**: Suppose you bet \$1 on Cleopatra to win the race with 4:1 odds. You will win \$4 if Cleopatra wins, otherwise you've lost \$1. 
- The amount you win (\$4) is determined so that you break even in the long run. 
- Suppose 5 identical races are run. In 1 of those races, Cleopatra wins, and in the other 4, Cleopatra loses. If you bet \$1 on Cleopatra in each race, you will lose that \$1 4 of 5 times. So, in order for you to break even, the designated amount you'll win when Cleopatra wins is \$4.
- This is a statistical concept known as *expected value*. Your expected value when placing the bet is \$0. We compute expected value by multiplying each possible outcome value by its probability and adding them all together:
\begin{align*}
\$4\cdot P(Y) + (-\$1) \cdot P(\overline{Y}) & = 0 \\
\$4\cdot 0.2 + (-\$1) \cdot 0.8 & = 0 \\
\$0.8 - \$0.8 & = 0
\end{align*}

### Probability Math

Up until now, we have only considered one event, $Y$. Now, suppose we have another event that we are interested in, $Z$. 

Let's consider the possibility of *either* of these two events, $Y$ and $Z$, occurring. We'd write this as $Y \cup Z$, which is mathematical notation for "$Y$ or $Z$ occurs". There are two scenarios that arise: \vspace{.1in}

1. $Y$ and $Z$ cannot occur together: they are _________ __________ \vspace{.1in}
2. $Y$ and $Z$ can occur together. 

In scenario \#1, computing the probability of either $Y$ or $Z$ happening is easy: we just add their respective probabilities together: 

$$ Y,Z \text{ mutually exclusive } \Rightarrow P(Y \cup Z) = P(Y) + P(Z)$$ 

In scenario \#2, computing the probability of either $Y$ or $Z$ happening is more complicated because we know there is a chance that $Y$ and $Z$ can happen together. We'd write this as $Y \cap Z$, which is mathematical notation for "$Y$ and $Z$ occurs". In scenario \#1, this event never occurred, so $P(Y \cap Z) = 0$ there. To compute the probability of $Y$ or $Z$ occurring in scenario \#2, we have to consider the probability of $Y$, the probability of $Z$, and the probability of $Y \cap Z$. If we just add $P(Y) + P(Z)$ as in scenario \#1, we include the event $Y \cap Z$ twice, so we have to subtract one instance of it: 

$$ Y,Z \text{ not mutually exclusive } \Rightarrow P(Y \cup Z) = P(Y) + P(Z) - P(Y \cap Z).$$ 

This probability is much easier to think about when illustrated. In Figure \ref{fig:bloodvendiag}, we consider human blood types. There are four groups: A, B, O, and AB, and there are two RH types: $+$ and $-$. We first consider the blood types A and B, represented by the two non-overlapping circles. Define: 

- Event $Y$ = a person has blood type A
- Event $Z$ = a person has blood type B
- Event $Y \cup Z$ = a person has blood type A or blood type B

These two events are *mutually exclusive* because one person cannot have both blood type A and blood type B. (The circles don't overlap in the venn diagram) So, the probability that a randomly selected person has blood type A or B is:

$$ P(Y \cup Z) = \_\_\_ \quad + \quad \_\_\_$$

```{r bloodvendiag, echo=FALSE, out.width='.5\\linewidth', fig.align='center', fig.cap='Probabilities of blood types in humans', fig.pos="h"}
knitr::include_graphics('img/bloodvenndiag.png')
```

Return to Figure \ref{fig:bloodvendiag} and consider two other events: a person having blood type A or having the Rh factor (RH+). We see in Figure \ref{fig:bloodvendiag} that someone can have both type A blood and the Rh factor (blood type A+). Define: 

- Event $Y$ = a person has blood type A
- Event $Z$ = a person has the Rh factor 
- Event $Y \cup Z$ = a person has blood type A or the Rh factor
- Event $Y \cap Z$ = a person has blood type A and the Rh factor (they have A+ blood)

So, the probabilty that someone has either type A blood or has the Rh factor is the sum of probability of having type A blood (represented by the yellow circle) and the probability of having the Rh factor (represented by the red rectangle) minus the probability of having A+ blood (represented by the orange area of overlap that is counted twice) in Figure \ref{fig:bloodvendiag}. So, the probability that a randomly selected person has blood type A or the Rh factor is:

$$ P(Y \cup Z) = \_\_\_ \quad + \quad \_\_\_ \quad - \quad \_\_\_$$

### Conditional Probability

Let's consider an event of interest $Y$ which has probability $P(Y)$. Then, suppose we learn of another event of interest $Z$ that has occurred. Knowing that $Z$ has occurred already may change our opinion about the likelihood of _____ occurring. The key idea here is that the probability of an event often depends on other information, leading us to the definition of *conditional probability*: 

$$ P(Y|Z), $$
which is the conditional ______________ that $Y$ occurs given that we know $Z$ has occurred. Return to Figure \ref{fig:bloodvendiag}. Suppose we want to know the probability of a person having type A blood, represented by the yellow circle. But, if we already know that a person has the Rh factor, we are only interested in the part of the type A circle that overlaps with the Rh+ rectangle. Thus the probability of having type A blood is different with different knowledge. The formula for calculating conditional probability is:

\begin{equation}\label{eq:2}
P(Y|Z) = \frac{P(Y\cap Z)}{P(Z)}
\end{equation} 

Returning to the venn diagram, the value $P(Y \cap Z)$ is represented by the overlap of the type A circle and the Rh+ rectangle, and the value $P(Z)$ is represented by the Rh+ rectangle. Then, the value $P(Y|Z)$ is the ratio of the overlap (A+) to the Rh+ rectangle. 

Equation \ref{eq:2} also gives us a multiplication rule for computing probabilities: 

\begin{equation}\label{eq:3}
P(Y\cap Z) = P(Y|Z) \cdot P(Z)
\end{equation} 

Philosophically speaking, it can be helpful to think of *all* probabilities as conditional. It is just a question of what information is assumed to be ___________. 

#### Examples

**Death Penalty Convictions**

A study of sentencing of 362 black people convicted of murder in Georgia in the 1980s found that 59 were sentenced to death (@baldus). They also examined the race of the murder victim, either black or white, and found some disparities. In Table \ref{tab:dp}, DP means the defendant received the death penalty, NDP means the defendant did not receive the death penalty. The race of the victim (RV) is either black (B) or white (W). 

\begin{table}
\centering
\begin{tabular}{l|cc|r}
RV & DP & NDP & Total \\
\hline
W & 45 & 85 & 130 \\
B & 14 & 218 & 232 \\
\hline
Total & 59 & 303 & 362
\end{tabular}
\caption{\label{tab:dp}The results of the Baldus et al study for black defendants convicted of murder.}
\end{table}

Returning to Section \@ref(definitions), let's define the problem: 

- **Population**: All black people convicted of murder in Georgia in the 1980s
- **Sample**: N/A (the whole population was studied)

Using the numbers from Table \ref{tab:dp}, compute the following probabilities: 

- $P(DP) = \frac{\quad}{\quad} = 0.\_\_\_$ \vspace{.1in}
- $P(DP | RV = W) = \frac{\quad}{\quad} = 0.\_\_\_$ \vspace{.1in}
- $P(DP | RV = B) = \frac{\quad}{\quad} = 0.\_\_\_$ \vspace{.1in}

Note: These numbers are selected from the study, and should not be considered a comprehensive summary of its results. There are a number of things not discussed here. The entire publication can be found online^[http://scholarlycommons.law.northwestern.edu/cgi/viewcontent.cgi?article=6378&context=jclc.] 

**Consecutive Matching Striae**

In firearms and toolmark analysis, the number of consecutive matching striae (CMS) between a crime scene sample and a lab sample is often used to help determine a match. Generally speaking, the higher the maximum number of CMS found in a pair, the more likely the two samples came from the same source.  Several known match (KM) pairs and known non-match (KNM) pairs of bullets were examined, and the results are shown in Figure \ref{fig:cms} (@hare). What is the probability of seeing two known matches (or two known non-matches) given the maximum number of CMS? Here, we condition on __________________________. Again, we briefly return to Section \@ref(definitions), let's define the problem: 

- **Population**: All pairs of fired bullets from unknown sources
- **Sample**: A sample of pairs of known matches and known non-matches

```{r cms, echo=FALSE, fig.align='center', fig.pos="h", out.width='.6\\linewidth', fig.cap="This bar chart represents the conditional probabilities of two bullets matching given the maximum number of CMS. The light blue represents known matches, while the dark blue represents known non-matches."}
library(ggplot2)
bstats <- read.csv("dat/bullet-stats-old.csv")
bstats$cmstop <- bstats$CMS
#bstats$cmstop[bstats$CMS >=20] <- "20+"
bstats$cmstop <- factor(bstats$cmstop)
bstats$cmstop <- reorder(bstats$cmstop, bstats$CMS)
bstats$km <- c( "KNM", "KM")[as.numeric(bstats$match)+1]
bstats$km <- as.factor(bstats$km)
ggplot(data=subset(bstats, !flagged)) + 
  geom_bar(aes(x=cmstop, fill=km), position=position_fill(reverse = TRUE)) +
  theme(legend.position="bottom") + 
  scale_fill_brewer("", palette="Paired") +
  xlab("maximum CMS") + ylab("Proportion") +
  theme(text = element_text(size=12))  
```

Generally, as seen in Figure \ref{fig:cms}, the probability of finding a match tends to increase with then number of maximum CMS. For _____ maximum CMS values is it much more likely that we have a __________________ pair.

### Independence

If the likelihood of one event is *not* affected by knowing whether a second has occured, then the two events are said to be __________________. For example, the region of the country where you live and what color car you drive are (probably) not related. 

The death penalty example from the previous section demonstrates that defendants receiving the death penalty is *not* independent of the race of the victim. In other words, a black defendant found guilty of murder in Georgia in the 1980s received a different penalty according to the race of the victim.

Another example from DNA analysis relies on on independence across chromosomes. By using loci on different chromosomes, there is independence between the allele counts, allowing for simple calculation of random match probabilities.  

### Probability Math...Again

Recall Equation \ref{eq:3}, which gives us the probability of two events, $Y$ and $Z$ occurring together: 

$$ P(Y \cap Z) = P(Z)\cdot P(Y|Z) = P(Y) \cdot P(Z|Y) $$

If $Y$ and $Z$ are *independent*, there is a simple formula: 

$$P(Y \cap Z) = \_\_\_\_ \cdot \_\_\_\_$$

This is because $Z$ occurring does not effect the probability of $Y$ occurring, and vice versa. Thus, 

$$ P(Y|Z) = P(Y) \quad \text{and} \quad P(Z|Y) = P(Z)$$

For example, the probability of being left-handed and from Florida is equal to the probability of being left-handed times the probability of being from Florida, assuming the events "being left-handed" and "being from Florida" are independent. 

Multiplying probabilities of events directly like this is *only* applicable when the events are independent. When *dependent* events are treated as independent events, things can go terribly wrong. An infamous example of this in the courts is the case \textit{People v. Collins} (@pvcollins). This was a robbery trial, where eyewitnesses described the robbers a "black male with a beard and a moustache, and a white female with a blonde ponytail, fleeing in a yellow car". 

The prosecution provided estimated probabilities of each of these individual characteristic: 

- P(black man with a beard) = ____________
- P(black man with a moustache) = ____________
- P(white woman with ponytail) = ____________
- P(white woman with blonde hair) = ____________
- P(yellow car) = ____________
- P(interratial couple in a car) = ____________

A mathematics "expert" talked about the so-called "multiplication rule for probability", and directly multiplied the above probabilities together without considering that the events could be *dependent*. i.e. a man with a beard probably has a much higher chance of having a moustache than a man with no beard. Due to this faulty math, the conviction was set aside and the statistical reasoning criticized for ignoring dependence among the characteristics. 

In a courtroom situation, let $S$ be the event that the suspect was present at the scene of the crime and $\overline{S}$ be the event that the suspect was not present at the scene. Assume that each juror has in mind an initial probability for the events $S$ and $\overline{S}$. Then, a witness says they saw a tall Caucasian male running from the scene, and the defendant is a tall Caucasian male. After hearing the witness' testimony, the jurors _________ their probabilities. Next, an expert witness testifies that fragments from a window broken during the crime and fragments found on the defendant's clothing match.  Again, the jurors update their ____________.  This process continues throughout the trial. There are some key questions to consider: 

- How should jurors update their probabilities?
- Do jurors *actually* think this way?

### Bayes' Rule

*Bayes' Rule* provides an _________ formula for probabilities. Like in the trial scenario above, suppose we have an initial estimate for the probability of event $S$, $P(S)$. Then, we learn that an event $R$ has occurred and we want to update or probability of event $S$. To do this, we need to know about the ____________ of $R$ and $S$. To update the probability of $S$, we can use Bayes' Rule, also called Bayes' ___________:

\begin{equation}\label{eq:4}
\begin{split}
P(S|R) & = \frac{P(R \cap S)}{P(R)} = \frac{P(R|S)P(S)}{P(R)} \\
& = \frac{P(R|S)P(S)}{P(R|S)P(S) + P(R|\overline{S})P(\overline{S})} 
\end{split}
\end{equation}

```{r probtree, echo = FALSE, fig.align='center', fig.cap="A probability tree showing the direction of flow when updating probabilities. Move from left to right on the tree. Events are in boxes.", out.width='.7\\linewidth'}
dat <- data.frame(id = c("", "R", "not R", "S", "not S", "S", "not S"),
                  x = c(0,1,1,2,2,2,2), 
                  y = c(0,1,-1,1.5,.5,-.5,-1.5), 
                  stringsAsFactors = FALSE)
edges <- data.frame(from = c("","", "R", "R", "not R", "not R"), 
                    to = c("S", "not R", "S", "not S", "S", "not S"),
                    from.x = c(0,0,1,1,1,1),
                    from.y = rep(c(0,1,-1), each = 2),
                    to.x = c(1,1,2,2,2,2), 
                    to.y = c(1,-1,1.5,.5, -.5, -1.5),
                    stringsAsFactors = FALSE)

labs <- data.frame(x = c(.5, 1.5, 1.5, .5, 1.5, 1.5),
                   y = c(.5, 1.25, .5, -.75, -.75, -1.5),
                   lab = c("P(R)", "P(S | R)", "P(not S | R)", "P(not R)", "P(S | not R)", "P(not S | not R)"),
                    stringsAsFactors = FALSE)

ggplot() + 
  geom_segment(data = edges, aes(x = from.x, y = from.y, xend = to.x, yend = to.y)) + 
  geom_label(data = dat, aes(x=x, y=y, label=id)) + 
  geom_text(data = labs, aes(x=x, y=y, label = lab), vjust = -1) + 
  theme_void()
```

#### Examples 

Consider performing diagnostic tests for gunshot residue. 

- Let $G$ denote the presence of gunshot residue \vspace{.1in}
- Let $\overline{G}$ denote the ____________ of gunshot residue \vspace{.1in} 
- Let $T$ denote a _____________ diagnostic test \vspace{.1in}
- Let $\overline{T}$ denote a negative diagnostic test

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|}
\hline
Truth & $T$ & $\overline{T}$ \\
\hline
$G$ & True Positive & False Negative \\
\hline
$\overline{G}$ & False Positive & True Negative \\
\hline
\end{tabular}
\caption{\label{tab:bayesex} All potential outcomes of a diagnostic test for gunshot residue.}
\end{table}

The values in the table can also be thought of as conditional probabilities: 

- The value $P(T|G)$ is the _______________________ rate, also called *sensitivity* of the test \vspace{.1in}
- The value $P(\overline{T}|\overline{G})$ is the ____________________ rate, also called the *specificity* of the test \vspace{.1in}
- The value $P(T|\overline{G})$ is the ______________________ rate, the Type I error rate \vspace{.1in} 
- The value $P(\overline{T}|G)$ is the ________________________ rate, the Type II error rate 

Studies of the diagnostics test usually tell us $P(T|G)$,  ___________,  and $P(\overline{T}|\overline{G})$, ___________ . Examiners may begin with some idea of $P(G)$, or the ___________ of gunshot residue in a similar situation. What is most relevent for the case is the *postitive predictive value*, or in probability notation, ___________. We can use __________________________ to obtain this value: 

$$ P(G|T) = \frac{P(T|G)P(G)}{P(T|G)P(G) + P(T|\overline{G})P(\overline{G})}$$

Generally speaking, the most important thing to remember is that, in general, $P(T|G) \quad \_\_\_\_ \quad P(G|T)$. 

The careful application of Bayes' Rule can sometimes lead to surprising, non-intuitive results. Continuing with the gunshot residue test example, assume 

- sensitivity is 98% ($P(\quad|\quad) = 0.98$) \vspace{.1in}
- specificity is 96% ($P(\quad|\quad) = 0.96$) \vspace{.1in}
- prevalence is 90% ($P(\quad) = 0.90$) \vspace{.1in}
- Plug values into the Bayes' Rule formula to find $P(G|T)$:
\begin{equation}\label{eq:5}
\begin{split}
 P(G|T) & = \frac{P(T|G)P(G)}{P(T|G)P(G) + P(T|\overline{G})P(\overline{G})} \\ 
 & = \frac{0.98 \cdot 0.9}{0.98 \cdot 0.9 + (1- 0.96)\cdot (1-0.9)} \\ 
 & = \frac{0.882}{0.882 + 0.004} \\
 & = 0.995
\end{split}
\end{equation}
- Now assume prevalence is 10% ($P(\quad) = 0.10$) and plug in the values again \vspace{.1in}
\begin{equation}\label{eq:6}
\begin{split}
 P(G|T) & = \frac{P(T|G)P(G)}{P(T|G)P(G) + P(T|\overline{G})P(\overline{G})} \\ 
 & = \frac{0.98 \cdot 0.1}{0.98 \cdot 0.1 + (1- 0.96)\cdot (1-0.1)} \\ 
 & = \frac{0.098}{0.098 + 0.036} \\
 & = \frac{0.098}{0.134} \\
 & = 0.731
\end{split}
\end{equation}
- So, even if there is a postive test, we are not really sure about whether gunshot residue is *actually* present.
- Why does this happen?? See Figure \ref{fig:probtree2}.

```{r probtree2, echo = FALSE, fig.align='center', fig.pos="h", fig.cap="A probability tree showing the direction of flow when updating probabilities for the presence of gunshot residue. Suppose there are 1,000 people in the population you're considering. Write the number of people in the groups throughout the tree according to the probabilities indicated on the branches of the tree", out.width='.7\\linewidth'}
dat <- data.frame(id = c("1000\npeople", "G", "not G", "T", "not T", "T", "not T"),
                  x = c(0,1,1,2,2,2,2), 
                  y = c(0,1,-1,1.5,.5,-.5,-1.5), 
                  stringsAsFactors = FALSE)
edges <- data.frame(from = c("1000\npeople","1000\npeople", "G", "G", "not G", "not G"), 
                    to = c("T", "not T", "T", "not T", "T", "not T"),
                    from.x = c(0,0,1,1,1,1),
                    from.y = rep(c(0,1,-1), each = 2),
                    to.x = c(1,1,2,2,2,2), 
                    to.y = c(1,-1,1.5,.5, -.5, -1.5),
                    stringsAsFactors = FALSE)

labs <- data.frame(x = c(.5, 1.5, 1.5, .5, 1.5, 1.5),
                   y = c(.5, 1.25, .5, -.75, -.75, -1.5),
                   lab = c("0.10", "0.98", "0.02", "0.90", "0.04", "0.96"),
                    stringsAsFactors = FALSE)

ggplot() + 
  geom_segment(data = edges, aes(x = from.x, y = from.y, xend = to.x, yend = to.y)) + 
  geom_label(data = dat, aes(x=x, y=y, label=id)) + 
  geom_text(data = labs, aes(x=x, y=y, label = lab), vjust = -1) + 
  xlim(c(-.1, 2.1)) + 
  theme_void()
```

### Bayes' Rule to the Likelihood Ratio

In the general forensic setting, let $S$ denote the event that the evidence from the scene and comparison sample are from the same source. Let $E$ denote the evidence found at the scene. The formulation of Bayes' Rule for this situation is: 

$$ P(S|E) = \frac{P(E|S)P(S)}{P(E|S)P(S) + P(E|\overline{S})P(\overline{S})}$$

We can rewrite Bayes' Rule in terms of odds: 

\begin{equation}\label{eq:odds}
\frac{P(S|E)}{P(\overline{S}|E)} = \frac{P(E|S)}{P(E|\overline{S})}\frac{P(S)}{P(\overline{S})}
\end{equation}

Derivation of Equation \ref{eq:odds} is shown in Equation \ref{eq:7}. For now, just consider Equation \ref{eq:odds}: 

- On the left, $\frac{P(S|E)}{P(\overline{S}|E)}$ are the odds in favor of $S$ given the evidence $E$. 
- The last term on the right, $\frac{P(S)}{P(\overline{S})}$ are the odds in favor of $S$ before seeing the evidence $E$ (the "prior odds") \vspace{.1in}
- The first term on the right $\frac{P(E|S)}{P(E|\overline{S})}$, is known as the _____________ ratio \vspace{.1in}
- The likelihood ratio (LR) is the factor by which we ___________ prior odds of two samples being from the same source to get ______________ odds (after seeing evidence) of the same source. 

\begin{equation}\label{eq:7}
\begin{split}
P(S|E) & = \frac{P(E|S)P(S)}{P(E|S)P(S) + P(E|\overline{S})P(\overline{S})} \\
\Rightarrow \frac{1}{P(S|E)} & = \frac{P(E|S)P(S) + P(E|\overline{S})P(\overline{S})}{P(E|S)P(S)} \\ 
  & = 1 + \frac{P(E|\overline{S})P(\overline{S})}{P(E|S)P(S)} \\ 
\Rightarrow \frac{1}{P(S|E)} -1 & = \frac{P(E|\overline{S})P(\overline{S})}{P(E|S)P(S)} \\ 
\frac{1}{P(S|E)} - \frac{P(S|E)}{P(S|E)} & = \\
\frac{1-P(S|E)}{P(S|E)} & = \\
\frac{P(\overline{S}|E)}{P(S|E)} & =  \frac{P(E|\overline{S})P(\overline{S})}{P(E|S)P(S)} \\
\Rightarrow \frac{P(S|E)}{P(\overline{S}|E)} & = \frac{P(E|S)P(S)}{P(E|\overline{S})P(\overline{S})} 
\end{split}
\end{equation}

#### Examples

Return to the gunshot residue (GSR) test example. Define:

- $E$ = evidence = a positive test for (GSR)
- $S$ = suspect has GSR on them 

$$LR = \frac{P(E|S)}{P(E|\overline{S})} = \frac{0.98}{0.04} = 24.5$$

In a high prevalence case ($P(G)=0.9$), the prior odds are $\frac{0.9}{0.1} = 9$. The posterior odds are $LR \times$prior odds $= 24.5\times 9 = 220.5:1$.

In a low prevalence case ($P(G)=0.1$), the prior odds are $\frac{0.1}{0.9} = \frac{1}{9}$. The posterior odds are $LR \times$prior odds $= 24.5\times \frac{1}{9} = 24.5:9 = 2.72:1$. 

We can also compute the likelihood ratio if the evidence were a negative test. This value turns out to be $\frac{1}{48}$, which is **not** the reciprocal of the LR for the positive test.

### Recap

- Probability is the _____________ language of _____________ \vspace{.1in}
- Provides a common scale, from _____ to _____, for describing the chance that an event will occur \vspace{.1in}
- **Conditional** probability is a key concept! The probabilitity of an event depends on what ____________ is available \vspace{.1in}
- Independent events can be powerful! They allow us to _________ probabilities of events *directly*, as is common in _________________. \vspace{.1in}
- ____________________ is a mathematical result showing how we should ___________ our probabilities when available information changes. \vspace{.1in}
    * This will later lead us to the likelihood ratio as a numerical ____________ of the evidence. \vspace{.1in}
    * Bayes' Rule does not necessarily describe how people operate in practice.
    
### Probability and the Courts

Sally Clark was the only person in the house when her first child died unexpectedly at 3 months old. The cause of death was determined to be SIDS, sudden infant death syndrome. One year later, Sally and her husband had a second child, who died at 2 months old under similar circumstances.  Sally was convicted of murder. 

During her trial, a pediatrician testified that the probability of a single SIDS death for a family like the Clarks (similar income, etc.) was $\frac{1}{8500}\approx 0.0001$, and thus the probability of two SIDS death in the family was $\frac{1}{8500^2} = \frac{1}{73 \times 10^6} \approx 1.37 \times 10^{-8}$. There are several problems with this approach to evidence. What do you think? Jot down a few ideas below: 

__________________________________________________________________________________________________________________________

\vspace{.1in}
__________________________________________________________________________________________________________________________
\vspace{.1in}
__________________________________________________________________________________________________________________________

\vspace{.1in} 

Issues with the evidence presented by the pediatrician: 

1. Is the probability of a child dying of SIDS given, $\frac{1}{8500}$, correct for "families like the Clarks"?
2. The use of direct multiplication of probabilities assumes independence of the two deaths in the family. (Independence within the family is not a reasonable assumption.)
3. Alternative hypotheses (causes of death of the infants) were not considered. Did something else with perhaps a higher likelihood cause the children's deaths? 

## Probability to Statistical Inference

Probability is important, but it is only one tool in our toolbox. Another, more powerful tool is statistical inference.

### Collecting Data

First, we consider data collection. Where do data come from? One data source is an *experiment*. An investigator designs a study and collects information on and maybe applies treatments to a *sample*, a subset of the population of interest. _________ can tell us a great deal about how to design an ___________ or choose a _________. 

The area of statistics concerned with creating studies is called *experimental design*. The experimental design literature is extensive (see for example @doe). Here are a few crucial points: 

- The goal of an experiment is to compare ____________ \vspace{.1in}
- Those ____________ must be ___________ assigned to units \vspace{.1in}
- The ______________ in the experiment must be large enough t obe able to make informed conclusions
- Blinding plays an important role in avoiding _______. e.g. "double-blind" studies in medicine, where neither the patient nor the doctor administering the treatment know which treatment the patient is receiving

How is experimental design relevant to forensic science? 

- Experiments are used to evaluate process improvements
- Blinding is used in "black box" studies, where examiners do not know ground truth 

Experiments almost always involve *sampling* from the population of interest. Why? 

- We sample because it is too _________ or _____________ to study the *entire* population \vspace{.1in}
- A _________ sample allows us to use the laws of ___________ to describe how certain we are that our ____________ answer reflects the ____________.
- There are many famous failures (cautionary tales) with __________________ sampling. (See Figure \ref{fig:dewey}.)

How is sampling relevant to forensic science? 

- Sampling techniques used to determine which and how many bags of suspect powder collected from a crime scene to test.

```{r dewey, echo=FALSE, out.width='.5\\linewidth', fig.align='center', fig.cap='This picture from the US presidential election of 1948 shows President Harry Truman, who won the election, holding a newspaper that went to print with the headline "Dewey Defeats Truman!" The headline was based on biased sampling that favored typically Republican demographics. Image Source: https://blogs.loc.gov/loc/2012/11/stop-the-presses/', fig.pos="h"}
knitr::include_graphics('img/dewey.jpg')
```

All data collected can be divided into one of two groups: qualitative or quantitative. 

- **Qualitative** data describe qualities about the observations. For example, the race of a suspect, or their level of education. There are two subcategories of qualitative data: \vspace{.1in}
    * ______________: the data belong to one of a discrete number of groups or categories. For example: blood type (A, B, AB, or O) \vspace{.1in}
    * ______________: the data belong to one group in a set of ordered values. For example, the evaluation of a teacher (poor, average, excellent). The categories have an inherent ordering, unlike in categorical data.\vspace{.1in}
- **Quantitative** data describe quantities that can be measured on the ovservations. These are numerical observations. There are also two subcategories of quantitative data: \vspace{.1in}
    * _______________: the values are distinct or separate. An easy-to-understand example is integer observations: $\{0,1,2,3,4, \dots \}$. A forensic science example is consecutive matching striae on bullets or toolmarks. (See Figure \ref{fig:cms})
    * ________________: the values can take on any value in a finite or infinite interval. Continuous values fall anywhere on the number line. A forensic science example is the refractive index of a glass fragment.

### Probability Distributions

Suppose we are to collect data on some characteristic for a sample of individuals or objects (e.g. weight, trace element concentration). A probability ___________ is used to describe these possible values and how ___________ each value is to occur. There are many, many possible probability distributions, but some of the most common are the Binomial, Poisson, Normal, and Lognormal distributions. The probabilities associated with each of these distributions and their possible outcomes are plotted in Figures
<!-- \ref{fig:binom}-\ref{fig:lognorm}.  -->

- Discrete distributions \vspace{.1in}
    * _________________: counts the number of __________ in a fixed number ($n$) of ___________. Possible values are $\{0,1,2, \dots, n\}$. \vspace{.1in}
    * ______________: counts the number of __________ occurring. Possible values are $\{0,1,2,3,4,5,\dots\}$
    
```{r binompois, echo=FALSE, message = FALSE, fig.align='center', fig.pos='h', fig.cap='On the left, the probability of each possible outcome for variable with binomial distribution with 10 trials and probability of success 0.75. On the right, the probability of each possible outcome for variable with Poisson distribution with mean value 5.', out.width='.49\\linewidth', fig.show='hold'}
library(ggfortify)
Binom <- data.frame(x = 0:10,
                    y = dbinom(0:10, 10, .75))
Pois <- data.frame(x = 0:20,
                    y = dpois(0:20, 5))
ggplot(data= Binom) + 
  geom_bar(aes(x = x, weight = y)) + 
  scale_x_continuous(labels = 0:10, breaks = 0:10)  +
  labs(x = "Number of successes", y = "Probability", 
       title = "Binomial(10,0.75)") + 
  theme_minimal()
ggplot(data= Pois) + 
  geom_bar(aes(x = x, weight = y)) + 
  scale_x_continuous(labels = 0:20, breaks = 0:20)  +
  labs(x = "Number of events observed", y = "Probability", 
       title = "Poisson(5)") + 
  theme_minimal()
```

- Continuous distributions \vspace{.1in}    
    * _______________: the famous, symmetric "bell-shaped" _________. Possible values are all real numbers, $(-\infty, \infty)$ 
    * ______________: the (natural) logarithm of observations from this distribution follow a __________ distribution. Possible values are all positive real numbers, $(0,\infty)$ 
    
```{r normlnorm, echo=FALSE, message=FALSE, fig.align='center', fig.pos='h', fig.cap="On the left, the probability distribution curve of possible outcomes for a variable with Normal distribution with mean value 100 and standard deviation 15. On the right, the probability distribution curve of possible outcomes for a variable with Lognormal distribution with mean value 0 and standard deviation 1.", out.width='.49\\linewidth', fig.show='hold'}
ggdistribution(dnorm, seq(55, 145, 1), mean = 100, sd = 15) + 
  scale_x_continuous(breaks = seq(60,140,20), 
                     labels = seq(60,140,20)) + 
  theme_minimal() + 
  labs(x = "", y = "Density", title = "Normal(100,15)")

ggdistribution(dlnorm, seq(0,10,.05), meanlog = 0, sdlog = 1) +
  scale_x_continuous(breaks = seq(0,10,2), 
                     labels = seq(0,10,2)) + 
  theme_minimal() + 
  labs(x = "", y = "Density", title = "LogNormal(0,1)")
```

#### Normal

You may already be familiar with this distribution with the bell-shaped curve. Measurement error is one example of something often assumed to follow a normal distribution. The normal distribution is described by two parameters: the ________, denoted by $\mu$, and the ____________________, denoted by $\sigma$. If we have a variable (say, something observed in our data like weight), we give the variable a capital letter, typically $X$. If this variable $X$ is normally distributed with mean $\mu$ and standard deviation $\sigma$, we write this as:  

$$ X \sim N(\_\_\_, \_\_\_) $$

In measurement error, for example, we typically assume that the mean is 0. So, if $X$ represents measurement error, we'd write $X \sim N(0,\sigma)$. 

There are many nice properties of the normal distribution. For instance, we know that _____% of observable values lie within _____ standard deviations of the mean ($\mu \pm 2\sigma$), and also that _____% of observable values lie within _____ standard deviations of the mean ($\mu \pm 3\sigma$). When working with the normal distribution, we use software (such as Excel, Matlab, R, SAS, etc.), tables^[See for example http://www.stat.ufl.edu/~athienit/Tables/Ztable.pdf], or websites like [onlinestatbook.com](http://onlinestatbook.com/2/calculators/normal_dist.html) or  [stattrek.com](http://stattrek.com/online-calculator/normal.aspx) to compute probabilities of events. 

```{r norm4, echo=FALSE, message=FALSE, fig.align='center', fig.pos='h', fig.cap='Four examples of the probability distribution functions for normally distributed variables', out.width='.49\\linewidth', fig.show='hold'}
ggdistribution(dnorm, seq(55, 145, 1), mean = 100, sd = 15) + 
  scale_x_continuous(breaks = seq(60,140,20), 
                     labels = seq(60,140,20)) + 
  scale_y_continuous(breaks = seq(0,.02, .01), labels = seq(0,.02, .01)) + 
  theme_minimal() + 
  labs(x = "IQ", y = "Density", 
       title = "IQ Scores are approximately Normal(100,15)")
ggdistribution(dnorm, seq(58, 82, .25), mean = 69, sd = 3) + 
  scale_x_continuous(breaks = seq(60,80,5), 
                     labels = seq(60,80,5)) + 
  scale_y_continuous(breaks = seq(0,.12, .04), labels = seq(0,.12, .04)) +  
  theme_minimal() + 
  labs(x = "Height (in)", y = "Density", 
       title = "Men's Heights (in) are approximately Normal(69,3)") 
ggdistribution(dnorm, seq(1.5198, 1.5202, .000005), mean = 1.52, sd = .00005) + 
  scale_x_continuous(breaks = seq(1.5198, 1.5202,.0001), 
                     labels = seq(1.5198, 1.5202,.0001)) + 
  scale_y_continuous(breaks = seq(0,8000, 2000), labels = seq(0,8000, 2000)) + 
  theme_minimal() + 
  labs(x = "Glass RI", y = "Density", 
       title = "Glass Refractive Index (RI) approximately Normal(1.52,0.00005)")
ggdistribution(dnorm, seq(-4, 4, .05), mean = 0, sd = 1) +
  scale_x_continuous(breaks = seq(-4,4,2), 
                     labels = seq(-4,4,2)) + 
  scale_y_continuous(breaks = seq(0,.4, .1), labels = seq(0,.4, .1)) +  
  theme_minimal() + 
  labs(x = "", y = "Density", 
       title = "Standard Normal(0,1)") 
```

#### Lognormal

We often act as if everything is normally distributed, but of course this is not true. For instance, a quantity that is certain to be ___________ (greater than or equal to zero) cannot possible be normally distributed. Consider trace element concetration: either none is detected, or there is some amount greater than 0 detected. 

In cases where nonnegative values are not possible, we may believe that the (natural) _________ of the quantity is normal, which gives us a __________ distribution for the quantity itself. The lognormal distribution, like the normal, has two parameters: mean (on the log scale), denoted _____, and standard deviation (on the log scale), denoted _____. 

```{r lognorm, echo = FALSE, message=FALSE, fig.align='center', fig.pos='h', fig.cap='Three lognormal distributions with the same mean (on the log scale) and different standard deviations (on the log scale)', out.width='.5\\linewidth'}
x <- seq(0, 3, .005)
sln <- dlnorm(x, meanlog=0, sdlog=1)
ln05 <- dlnorm(x, meanlog=0, sdlog=.5)
ln025 <- dlnorm(x, meanlog=0, sdlog=.25)
dat <- data.frame(x,sln,ln05,ln025)
library(tidyr)
dat %>% gather(dist, val, -x) %>% 
  ggplot() + geom_line(aes(x = x , y = val, color = dist)) + 
  theme_minimal() + 
  scale_color_discrete(labels = c("Lognormal(0,0.25)", "Lognormal(0,0.5)", "Lognormal(0,1)"), name = "") + 
  theme(legend.position = c(.75,.75), legend.background  = element_rect(fill = 'white', color = 'white'))
```

#### Discrete 

Some quantities take on very few possible values. These are *discrete* data. 

Recall the two common discrete distributions from section \@ref(probability-distributions): 

- Binomial: \vspace{.1in}
    * Data are _____ (two categories: "success" or "failure") \vspace{.1in}
    * Data are a result of $n$ independent ______  \vspace{.1in}
    * $P(\text{success}) = p$ on each trial. (Same ______ of success each time)  \vspace{.1in}
    * Expected number of successes you expect to see out of $n$ trials: _____ $\times$ _____  \vspace{.1in}
    * Example: Suspect a student of cheating on an exam, response is the number of correct answers.
- Poisson: \vspace{.1in}    
    * Data are counts: number of events occurring in a ______ time \vspace{.1in}
    * The mean and the ________ of this distribution are the same, so the variablility in responses increases as the _____ increases. \vspace{.1in}
    * Example: number of calls to 911 between 10:00 and midnight on Friday nights.
    
    
    
 

## Statistical Inference - Estimation

### Background

### Point Estimation

### Standard Errors

### Sample Size

### Interval Estimation


## Statistical Inference - Hypothesis Testing

### Background 

### Normal Data

### Confidence Intervals

### Comparing Two Means

### Discussion